{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/asmitpatidar/ludo-rules-assistant-with-gemini?scriptVersionId=235741343\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"2e2cae55","metadata":{"papermill":{"duration":0.007305,"end_time":"2025-04-23T22:07:05.091285","exception":false,"start_time":"2025-04-23T22:07:05.08398","status":"completed"},"tags":[]},"source":["# Ludo Rules Assistant with Gemini (RAG, Few-Shot, Func Calling, Grounding, Eval)\n","> **Made By: Asmit Patidar**"]},{"cell_type":"markdown","id":"b059bc64","metadata":{"papermill":{"duration":0.005832,"end_time":"2025-04-23T22:07:05.103625","exception":false,"start_time":"2025-04-23T22:07:05.097793","status":"completed"},"tags":[]},"source":["**Building an Intelligent Ludo Rules Assistant with Gemini**\n","\n","**Problem:** The board game Ludo, while popular, has numerous rules, variations, and specific situations (like captures, blocks, safe squares) that can be confusing for players, especially beginners. Getting quick, accurate, and easy-to-understand answers during a game can be challenging.\n","\n","**Solution:** This notebook demonstrates how to build an intelligent Ludo Rules Assistant using Google's powerful Gemini generative AI model. We aim to create an assistant that doesn't just give generic answers but provides accurate, context-aware, clearly explained rules based on a defined knowledge source.\n","\n","**Approach:** To achieve a high-quality assistant, we'll go beyond basic prompting and leverage several advanced GenAI techniques:\n","* **Retrieval-Augmented Generation (RAG):** Grounding the AI's answers in factual Ludo rules text.\n","* **Few-Shot Prompting:** Guiding the AI to respond in a consistent, clear, and helpful style suitable for explaining rules.\n","* **Function Calling:** Enabling the AI to use external tools for tasks like clarifying specific game terms.\n","* **Grounding Techniques:** Ensuring the AI strictly adheres to the provided rule context, minimizing inaccuracies.\n","* **GenAI Evaluation:** Assessing the quality of the assistant's responses using an AI-based judge.\n","\n","**Goal:** This notebook serves as a practical guide and template for implementing these techniques to build a robust and helpful domain-specific chatbot."]},{"cell_type":"markdown","id":"f77a8cfb","metadata":{"papermill":{"duration":0.005805,"end_time":"2025-04-23T22:07:05.116813","exception":false,"start_time":"2025-04-23T22:07:05.111008","status":"completed"},"tags":[]},"source":["--------------------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","id":"b07a8074","metadata":{"papermill":{"duration":0.005733,"end_time":"2025-04-23T22:07:05.128656","exception":false,"start_time":"2025-04-23T22:07:05.122923","status":"completed"},"tags":[]},"source":["This notebook walks through creating an assistant that can answer questions about the board game Ludo. We'll use Google's Gemini model and integrate several powerful capabilities."]},{"cell_type":"markdown","id":"94f5479b","metadata":{"papermill":{"duration":0.005763,"end_time":"2025-04-23T22:07:05.140461","exception":false,"start_time":"2025-04-23T22:07:05.134698","status":"completed"},"tags":[]},"source":["# 1. Building an Intelligent Ludo Rules Assistant with Gemini: Setup and Dependencies"]},{"cell_type":"markdown","id":"8712742c","metadata":{"papermill":{"duration":0.005832,"end_time":"2025-04-23T22:07:05.152391","exception":false,"start_time":"2025-04-23T22:07:05.146559","status":"completed"},"tags":[]},"source":["First, its important to install the necessary library and configure your API key. It is **VERY important** that for Kaggle Kernels, you add your `GOOGLE_API_KEY` via the \"Secrets\" tab (Add-ons -> Secrets) rather than pasting it directly into the code."]},{"cell_type":"code","execution_count":1,"id":"7ec42346","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:05.16671Z","iopub.status.busy":"2025-04-23T22:07:05.165704Z","iopub.status.idle":"2025-04-23T22:07:11.673817Z","shell.execute_reply":"2025-04-23T22:07:11.672685Z"},"papermill":{"duration":6.517355,"end_time":"2025-04-23T22:07:11.67589","exception":false,"start_time":"2025-04-23T22:07:05.158535","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h"]}],"source":["!pip install -U -q \"google-genai==1.7.0\""]},{"cell_type":"markdown","id":"456a6ba1","metadata":{"papermill":{"duration":0.006379,"end_time":"2025-04-23T22:07:11.689462","exception":false,"start_time":"2025-04-23T22:07:11.683083","status":"completed"},"tags":[]},"source":["Next, I will import the SDK and some helpers to render the output"]},{"cell_type":"code","execution_count":2,"id":"0b1ad163","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:11.704267Z","iopub.status.busy":"2025-04-23T22:07:11.703892Z","iopub.status.idle":"2025-04-23T22:07:14.053365Z","shell.execute_reply":"2025-04-23T22:07:14.052185Z"},"papermill":{"duration":2.359135,"end_time":"2025-04-23T22:07:14.055298","exception":false,"start_time":"2025-04-23T22:07:11.696163","status":"completed"},"tags":[]},"outputs":[],"source":["import google.generativeai as genai\n","import os\n","import json\n","from datetime import datetime"]},{"cell_type":"markdown","id":"263b4ee1","metadata":{"papermill":{"duration":0.00608,"end_time":"2025-04-23T22:07:14.068043","exception":false,"start_time":"2025-04-23T22:07:14.061963","status":"completed"},"tags":[]},"source":["Now I will retrieve my Google API Secret Key from Kaggle Secrets"]},{"cell_type":"code","execution_count":3,"id":"70bd42cd","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.082546Z","iopub.status.busy":"2025-04-23T22:07:14.08185Z","iopub.status.idle":"2025-04-23T22:07:14.195124Z","shell.execute_reply":"2025-04-23T22:07:14.194169Z"},"papermill":{"duration":0.122254,"end_time":"2025-04-23T22:07:14.196753","exception":false,"start_time":"2025-04-23T22:07:14.074499","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["API Key configured successfully!\n"]}],"source":["from kaggle_secrets import UserSecretsClient\n","\n","GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","\n","# --- Fallback function to verify Google API Key is found ---\n","if GOOGLE_API_KEY:\n","    genai.configure(api_key=GOOGLE_API_KEY)\n","    print(\"API Key configured successfully!\")\n","else:\n","    print(\"API Key not configured. Please add it via Kaggle Secrets or environment variables.\")"]},{"cell_type":"markdown","id":"4ce09c9c","metadata":{"papermill":{"duration":0.006073,"end_time":"2025-04-23T22:07:14.209588","exception":false,"start_time":"2025-04-23T22:07:14.203515","status":"completed"},"tags":[]},"source":["# 2. Setting Up the Knowledge Foundation: Retrieval for RAG"]},{"cell_type":"markdown","id":"07e6ee63","metadata":{"papermill":{"duration":0.005992,"end_time":"2025-04-23T22:07:14.221866","exception":false,"start_time":"2025-04-23T22:07:14.215874","status":"completed"},"tags":[]},"source":["**The Core Idea (RAG):** To answer questions accurately about Ludo, our AI assistant can't rely solely on its general knowledge, which might be incomplete or contain variations. Retrieval-Augmented Generation (RAG) solves this by first *retrieving* relevant information from a trusted source (our Ludo knowledge base) and then providing this information to the Gemini model as context for *generating* the answer.\n","\n","**Step 1: The Knowledge Base (KB):** We need a source of Ludo rules. For demonstration purposes, we define a simple Python dictionary (`LUDO_RULES_KB`) containing rule snippets.\n","* **In a Real Application:** This would be replaced by a more robust system, like loading rules from text files or a database and storing them in a *vector database* (e.g., ChromaDB, FAISS, Pinecone) for efficient searching.\n","\n","**Step 2: The Retrieval Function:** We create a function (`retrieve_relevant_rules`) to search the KB based on the user's query. This function simulates the \"Retrieval\" part of RAG.\n","* **Our Simple Approach:** This notebook uses basic keyword matching for simplicity. It finds snippets containing words from the user's query.\n","* **For Better Results:** Real-world RAG uses *semantic search*. This involves converting the query and rule snippets into numerical representations (embeddings) and finding the snippets that are semantically closest in meaning to the query, leading to much more relevant results even if the exact keywords don't match.\n","\n","This retrieval step is crucial for providing the factual basis upon which Gemini will generate its grounded and informative answers."]},{"cell_type":"code","execution_count":4,"id":"bf74ec07","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.236672Z","iopub.status.busy":"2025-04-23T22:07:14.23601Z","iopub.status.idle":"2025-04-23T22:07:14.24369Z","shell.execute_reply":"2025-04-23T22:07:14.242697Z"},"papermill":{"duration":0.016508,"end_time":"2025-04-23T22:07:14.245163","exception":false,"start_time":"2025-04-23T22:07:14.228655","status":"completed"},"tags":[]},"outputs":[],"source":["LUDO_RULES_KB = {\n","    # --- Main gameplay mechanics ---\n","    \"objective\": \"The main goal of Ludo is to be the first player to move all four of their tokens (pieces) from their starting area, around the entire board track, and into the central home triangle.\",\n","    \"setup\": \"Each player chooses a color and places their four tokens in the corresponding starting circle (yard). The game board is set up with the track, starting squares, safe squares, and home columns.\",\n","    \"turn_sequence\": \"Players typically take turns in a clockwise direction. A player rolls a standard six-sided die to determine their move.\",\n","    \"die_roll\": \"A player rolls the die once per turn, unless they roll a 6, which grants an additional roll (bonus roll).\",\n","    \"movement\": \"Tokens move clockwise around the track according to the number rolled on the die. Players must use the full die roll if possible. If multiple moves are possible, the player chooses which token to move.\",\n","\n","    # --- Starting and entering the track ---\n","    \"start_piece\": \"To move a token from the starting circle (yard) onto the starting square on the track, a player must roll a 6. The starting square is the colored square just outside the starting circle. Rolling a 6 to start a piece also grants the player an additional bonus roll.\",\n","\n","    # --- Capturing opponent tokens ---\n","    \"capture\": \"If a player's token lands on a square occupied by a single opponent's token (not on a safe square), the opponent's token is captured. Captured tokens are returned to their owner's starting circle (yard) and must be brought back into play by rolling a 6.\",\n","    \"no_capture_on_safe\": \"A token cannot be captured if it is on a 'safe' square (often marked with a star or globe, or the player's own starting square).\",\n","    \"no_capture_own_color\": \"A player cannot capture their own tokens.\",\n","\n","    # --- Special rolls and squares ---\n","    \"rolling_six\": \"Rolling a 6 gives the player an extra turn (a bonus roll). A 6 must be rolled to move a token out of the starting circle. If a player already has all tokens in play, a 6 allows them to move any token 6 spaces.\",\n","    \"three_consecutive_sixes\": \"If a player rolls three consecutive 6s in a single turn, their turn ends immediately. Often, the token moved with the third 6 is sent back to the starting circle (this rule can vary).\",\n","    \"safe_squares\": \"Certain squares on the board are designated as 'safe squares'. These typically include the starting square for each color and squares marked with a star or globe. An opponent cannot capture a token resting on a safe square. Multiple tokens (even from different players) can occupy a safe square.\",\n","\n","    # --- Blocks ---\n","    \"blocks\": \"When two or more tokens of the same color occupy the same non-safe square, they form a 'block'. An opponent's token cannot pass or land on a square occupied by a block. Blocks cannot be formed on safe squares.\",\n","    \"moving_blocks\": \"A block can be moved forward together if the player rolls an even number. The die roll is divided by two, and the block moves that many spaces. For example, rolling a 4 allows the block to move 2 spaces. Some variations do not allow moving blocks.\",\n","\n","    # --- Reaching home ---\n","    \"home_column\": \"Each player has a coloured 'home column' leading to the central home triangle. Only tokens of that specific color can enter their corresponding home column.\",\n","    \"entering_home_column\": \"A token enters its home column based on its position on the main track and the die roll. It continues moving up the column towards the center.\",\n","    \"movement_in_home\": \"Movement within the home column requires an exact die roll to land on an empty square within the column or the final home triangle. Tokens cannot jump over other tokens within their own home column.\",\n","    \"exact_roll_to_finish\": \"A token must reach the final home triangle spot by an exact die roll count. If the roll is too high, the token cannot move into the triangle that turn (it might move back spaces in some variations, or simply not move).\",\n","\n","    # --- Winning the game ---\n","    \"winning\": \"The game is won when a player successfully moves all four of their tokens around the board and into their home triangle. The first player to achieve this wins.\",\n","\n","    # --- Optional Rules ---\n","    \"must_form_block_to_enter_home\": \"Some variations require a player to capture at least one opponent's token before any of their tokens can enter the home column.\",\n","    \"bounce_back_home\": \"In some rules, if a player rolls a number higher than needed to enter the home triangle, the token moves into the triangle and then bounces back the remaining number of steps.\",\n","\n","    # --- Clarification of terms ---\n","    \"ambiguous_term_block\": \"A 'block' in Ludo refers to two or more pieces of the same color on the same non-safe square. Opponents cannot land on or pass a block.\",\n","    \"ambiguous_term_capture\": \"'Capture' (or 'hit') means landing exactly on a square occupied by a single opponent's piece (not on a safe square), sending that piece back to its starting circle.\",\n","    \"ambiguous_term_home_column\": \"'Home column' (or 'home stretch') is the final colored path leading to the center triangle, accessible only by pieces of that color.\",\n","    \"ambiguous_term_yard\": \"'Yard' (or 'starting circle', 'base') is the area where a player's four tokens begin the game.\",\n","    \"ambiguous_term_token\": \"'Token' (or 'piece', 'pawn', 'man') refers to one of the four playing pieces each player controls.\"\n","}"]},{"cell_type":"code","execution_count":5,"id":"183a9a05","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.259684Z","iopub.status.busy":"2025-04-23T22:07:14.259376Z","iopub.status.idle":"2025-04-23T22:07:14.266079Z","shell.execute_reply":"2025-04-23T22:07:14.265165Z"},"papermill":{"duration":0.015397,"end_time":"2025-04-23T22:07:14.267574","exception":false,"start_time":"2025-04-23T22:07:14.252177","status":"completed"},"tags":[]},"outputs":[],"source":["def retrieve_relevant_rules(query, kb, num_results=3):\n","    query_words = set(query.lower().split())\n","    \n","    # Score snippets based on word overlap\n","    scores = {}\n","    for key, rule in kb.items():\n","        # Give slight boost to keys matching query words (like 'block')\n","        key_score_boost = 5 if key in query_words else 0\n","        rule_words = set(rule.lower().split())\n","        common_words = query_words.intersection(rule_words)\n","        # Simple scoring: count common words, boost if key matches\n","        scores[key] = len(common_words) + key_score_boost \n","        \n","    # Get top N results\n","    sorted_keys = sorted(scores, key=scores.get, reverse=True)\n","    \n","    relevant_snippets = [f\"- {kb[key]}\" for key in sorted_keys[:num_results] if scores[key] > 0]\n","\n","    # Debugging statement\n","    print(f\"DEBUG: Retrieved {len(relevant_snippets)} snippets for query '{query}'\")\n","    \n","    return \"\\n\".join(relevant_snippets) if relevant_snippets else \"No specific rules found matching the query keywords.\"\n"]},{"cell_type":"markdown","id":"d3043003","metadata":{"papermill":{"duration":0.006138,"end_time":"2025-04-23T22:07:14.28059","exception":false,"start_time":"2025-04-23T22:07:14.274452","status":"completed"},"tags":[]},"source":["# 3. Guiding the AI's Style: Few-Shot Prompting"]},{"cell_type":"markdown","id":"9ebaf9d0","metadata":{"papermill":{"duration":0.006141,"end_time":"2025-04-23T22:07:14.293118","exception":false,"start_time":"2025-04-23T22:07:14.286977","status":"completed"},"tags":[]},"source":["**The Challenge:** Simply giving the retrieved rules (context) and the user's question to the AI might result in answers that are too technical, poorly formatted, or inconsistent in tone. We want our Ludo assistant to be consistently clear, helpful, and explain rules in an easy-to-understand manner.\n","\n","**The Solution: Few-Shot Prompting:** This technique involves \"teaching by example\". We provide Gemini with a few (`few-shot`) examples directly within the prompt. Each example shows:\n","1.  A typical user query.\n","2.  The kind of relevant context retrieved for that query.\n","3.  The *ideal* answer format and style we want the assistant to emulate.\n","\n","**Why it Helps:**\n","* **Sets the Tone:** Guides the AI to be helpful and explanatory.\n","* **Ensures Format:** Encourages consistent answer structure.\n","* **Improves Clarity:** Demonstrates how to rephrase rules clearly.\n","* **Reinforces Grounding:** Examples implicitly show how to use *only* the provided context.\n","\n","By including these examples (`few_shot_examples`), we steer the model towards generating higher-quality, more user-friendly responses tailored to explaining Ludo rules effectively."]},{"cell_type":"code","execution_count":6,"id":"5d658c27","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.307506Z","iopub.status.busy":"2025-04-23T22:07:14.307141Z","iopub.status.idle":"2025-04-23T22:07:14.312804Z","shell.execute_reply":"2025-04-23T22:07:14.311823Z"},"papermill":{"duration":0.014737,"end_time":"2025-04-23T22:07:14.314316","exception":false,"start_time":"2025-04-23T22:07:14.299579","status":"completed"},"tags":[]},"outputs":[],"source":["few_shot_examples = [\n","    {\n","        \"query\": \"How do I get my pawn out of the start?\",\n","        \"context\": \"- To move a token from the starting circle onto the starting square on the track, a player must roll a 6.\\n- The starting square is the coloured square just outside the starting circle.\\n- Rolling a 6 earns the player an additional bonus roll.\",\n","        \"answer\": \"To move one of your pieces from your starting area onto the main track, you need to roll a 6 with the die. When you roll a 6, you place a piece on the colored square right outside your starting circle (your 'start square'). Rolling a 6 also grants you an extra roll.\"\n","    },\n","    {\n","        \"query\": \"What if I land exactly where another player's piece is?\",\n","        \"context\": \"- If a player's token lands on a square occupied by a single opponent's token, the opponent's token is captured.\\n- Captured tokens are returned to their owner's starting circle.\\n- A token cannot be captured if it is on a 'safe' square (e.g., star squares or squares with own color).\\n- Tokens of the same color can form a block if multiple are on the same square; blocks cannot be passed or captured by opponents.\",\n","        \"answer\": \"When your piece lands on the exact same square as an opponent's single piece, you capture it! Their piece goes back to their starting area, and they have to roll a 6 to bring it out again. However, you cannot capture pieces on 'safe' squares (like the ones marked with a star) or if the opponent has two or more pieces on the same square forming a 'block'.\"\n","    },\n","     {\n","        \"query\": \"How do blocks work?\",\n","        \"context\": \"- When two or more tokens of the same color occupy the same square, they form a 'block'. An opponent's token cannot pass or land on a square occupied by a block.\\n- A 'block' in Ludo refers to two or more pieces of the same color on the same square. Opponents cannot land on or pass a block.\",\n","        \"answer\": \"A 'block' is formed when you have two or more of your pieces on the same square. Opponents cannot land on a square with your block, nor can they move past it, even if their dice roll would normally take them further. Blocks are a good way to protect your pieces or hinder opponents.\"\n","    }\n","]"]},{"cell_type":"markdown","id":"d454f058","metadata":{"papermill":{"duration":0.00682,"end_time":"2025-04-23T22:07:14.327669","exception":false,"start_time":"2025-04-23T22:07:14.320849","status":"completed"},"tags":[]},"source":["# 4. Extending Capabilities: Function Calling"]},{"cell_type":"markdown","id":"60d6c3a2","metadata":{"papermill":{"duration":0.006723,"end_time":"2025-04-23T22:07:14.341068","exception":false,"start_time":"2025-04-23T22:07:14.334345","status":"completed"},"tags":[]},"source":["**The Challenge:** Sometimes, the retrieved rules might use specific Ludo jargon (like 'block', 'home column') that the user might not understand, or the AI might need specific, up-to-date information not directly in the static rules (though less common for Ludo rules).\n","\n","**The Solution: Function Calling:** Gemini's function calling feature allows the AI model to request that our application execute specific, pre-defined Python functions *during* the generation process. This lets the AI interact with external tools or code.\n","\n","**Our Implementation:**\n","1.  **Define Python Function:** We create a function (`clarify_ludo_term`) that can look up the definition of a specific Ludo term within our knowledge base.\n","2.  **Declare for Gemini:** We describe this function's purpose, name, and expected arguments (`parameters`) to the Gemini API using `FunctionDeclaration` and `Tool`. This tells Gemini *when* and *how* it can request this function to be called.\n","\n","**Why it Helps:**\n","* **Handles Ambiguity:** If Gemini detects potential confusion around a term based on the query and context, it can proactively call `clarify_ludo_term` to get a definition.\n","* **Provides Specific Tools:** Allows the AI to leverage dedicated logic for specific tasks beyond text generation.\n","* **Improves Robustness:** Makes the assistant more helpful by allowing it to resolve potential points of confusion dynamically.\n","\n","This adds a layer of interactivity and specialized capability to our Ludo assistant."]},{"cell_type":"code","execution_count":7,"id":"074118c2","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.35581Z","iopub.status.busy":"2025-04-23T22:07:14.355065Z","iopub.status.idle":"2025-04-23T22:07:14.360365Z","shell.execute_reply":"2025-04-23T22:07:14.3596Z"},"papermill":{"duration":0.013803,"end_time":"2025-04-23T22:07:14.361693","exception":false,"start_time":"2025-04-23T22:07:14.34789","status":"completed"},"tags":[]},"outputs":[],"source":["from google.generativeai.types import FunctionDeclaration, Tool\n","\n","def clarify_ludo_term(term: str):\n","    \"\"\"\n","    Provides a definition for potentially ambiguous Ludo terms based on the KB.\n","    Input:\n","        term: The Ludo term that needs clarification.\n","    Output:\n","        A JSON string containing the definition of the term or an error message.\n","    \"\"\"\n","    # Debugging Statement\n","    print(f\"DEBUG: Function Call - clarify_ludo_term(term='{term}')\")\n","    \n","    term_key = f\"ambiguous_term_{term.lower().replace(' ', '_')}\"\n","    definition = LUDO_RULES_KB.get(term_key, f\"Sorry, I don't have a specific definition for '{term}' in my Ludo knowledge base.\")\n","    \n","    return {\"term\": term, \"definition\": definition} "]},{"cell_type":"code","execution_count":8,"id":"e8b4ff15","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.376465Z","iopub.status.busy":"2025-04-23T22:07:14.375727Z","iopub.status.idle":"2025-04-23T22:07:14.381018Z","shell.execute_reply":"2025-04-23T22:07:14.38016Z"},"papermill":{"duration":0.014071,"end_time":"2025-04-23T22:07:14.382431","exception":false,"start_time":"2025-04-23T22:07:14.36836","status":"completed"},"tags":[]},"outputs":[],"source":["ludo_term_clarifier_func = FunctionDeclaration(\n","    name=\"clarify_ludo_term\",\n","    description=\"Looks up the definition of a specific Ludo game term (like 'block', 'capture', 'safe square', 'home column') if the user seems confused or asks for clarification based on the provided context.\",\n","    parameters={\n","        \"type_\": \"OBJECT\", # Use type_ instead of type\n","        \"properties\": {\n","            \"term\": {\n","                \"type_\": \"STRING\", # Use type_ instead of type\n","                \"description\": \"The specific Ludo term the user needs defined, e.g., 'block' or 'safe square'.\"\n","            }\n","        },\n","        \"required\": [\"term\"]\n","    },\n",")"]},{"cell_type":"code","execution_count":9,"id":"70117a87","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.397523Z","iopub.status.busy":"2025-04-23T22:07:14.396866Z","iopub.status.idle":"2025-04-23T22:07:14.401492Z","shell.execute_reply":"2025-04-23T22:07:14.400529Z"},"papermill":{"duration":0.01352,"end_time":"2025-04-23T22:07:14.402927","exception":false,"start_time":"2025-04-23T22:07:14.389407","status":"completed"},"tags":[]},"outputs":[],"source":["ludo_tool = Tool(function_declarations=[ludo_term_clarifier_func])"]},{"cell_type":"markdown","id":"bea6ef5e","metadata":{"papermill":{"duration":0.006258,"end_time":"2025-04-23T22:07:14.415952","exception":false,"start_time":"2025-04-23T22:07:14.409694","status":"completed"},"tags":[]},"source":["# 5. Ensuring Accuracy: Grounding the Assistant"]},{"cell_type":"markdown","id":"375a8ec2","metadata":{"papermill":{"duration":0.006551,"end_time":"2025-04-23T22:07:14.428914","exception":false,"start_time":"2025-04-23T22:07:14.422363","status":"completed"},"tags":[]},"source":["**The Problem: Hallucination:** Large Language Models (LLMs) can sometimes generate plausible-sounding but incorrect or fabricated information (\"hallucinate\"). For a rules assistant, accuracy is paramount – providing incorrect rules is worse than providing no answer.\n","\n","**The Solution: Grounding:** Grounding aims to tie the LLM's responses firmly to a reliable source of information. In our RAG setup, the primary source is the `Retrieved Context` obtained from our Ludo rules KB.\n","\n","**Our Strategy: Prompt-Based Grounding:** While advanced Gemini features exist, a highly effective method, especially combined with RAG, is through explicit instructions in the prompt:\n","1.  **Strict Instruction:** We clearly tell Gemini: \"Use the provided 'Retrieved Context' *strictly* to answer the 'User Query'.\"\n","2.  **Forbid External Knowledge:** We add: \"Do not use any external knowledge.\"\n","3.  **Handle Missing Information:** We instruct it on what to do if the answer isn't in the context: \"If the context doesn't contain the answer, state that clearly.\"\n","\n","**Why it Works:**\n","* **Leverages RAG:** RAG provides the specific, factual text to ground the answer in.\n","* **Directs the Model:** The prompt instructions focus the model's attention solely on the provided context.\n","* **Reduces Hallucination:** Minimizes the chance of the model inventing rules or using outdated general knowledge.\n","\n","This combination of RAG and careful prompting is key to building a trustworthy Ludo rules assistant."]},{"cell_type":"markdown","id":"72bcd564","metadata":{"papermill":{"duration":0.006167,"end_time":"2025-04-23T22:07:14.4415","exception":false,"start_time":"2025-04-23T22:07:14.435333","status":"completed"},"tags":[]},"source":["# 6. Assembling the Request: The Prompt Formatting Function"]},{"cell_type":"markdown","id":"29f5fad7","metadata":{"papermill":{"duration":0.006071,"end_time":"2025-04-23T22:07:14.453996","exception":false,"start_time":"2025-04-23T22:07:14.447925","status":"completed"},"tags":[]},"source":["**The Task:** Before we send the user's query to Gemini, we need to combine all the pieces we've prepared: the core instructions, the grounding directives, the few-shot examples (our style guide), the dynamically retrieved RAG context (the facts), and the actual user question.\n","\n","**The Solution: `format_prompt_with_all_features` Function:** This utility function acts as our \"prompt assembler\". It takes the current user query and the retrieved context as input, along with our pre-defined few-shot examples, and constructs the final, comprehensive prompt string.\n","\n","**Structure of the Final Prompt:**\n","1.  **System Instructions:** Role definition (Ludo expert), core task (answer based on context), grounding rules (use context strictly, state if not found), style guidance (follow examples).\n","2.  **Few-Shot Examples:** The formatted examples showing desired input/output pairs.\n","3.  **Current Task Marker:** Clearly indicates the start of the actual request.\n","4.  **User Query:** The specific question the user asked.\n","5.  **Retrieved Context:** The relevant rule snippets fetched by our RAG retriever.\n","6.  **Answer Cue:** Ends with `Ideal Answer:` to signal Gemini where to begin its generation.\n","\n","This structured prompt ensures that Gemini receives all the necessary information and guidance in a clear, organized manner to generate the best possible response according to our requirements."]},{"cell_type":"code","execution_count":10,"id":"f6ef80c2","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.53029Z","iopub.status.busy":"2025-04-23T22:07:14.529474Z","iopub.status.idle":"2025-04-23T22:07:14.535011Z","shell.execute_reply":"2025-04-23T22:07:14.534279Z"},"papermill":{"duration":0.014348,"end_time":"2025-04-23T22:07:14.53631","exception":false,"start_time":"2025-04-23T22:07:14.521962","status":"completed"},"tags":[]},"outputs":[],"source":["# Formats the final prompt including instructions, few-shot examples, and grounding.\"\"\"\n","def format_prompt_with_all_features(user_query, retrieved_context, examples):\n","    \n","    prompt_start = \"\"\"SYSTEM: You are an AI assistant expert in the board game Ludo. \n","Your task is to answer the user's query based *strictly* on the provided 'Retrieved Context'. \n","Do not add information not present in the context. If the context does not contain the answer, explicitly state that.\n","Emulate the clear and direct style shown in the 'Examples' section below.\n","\n","--- Examples ---\n","\"\"\"\n","    \n","    example_texts = []\n","    for ex in examples:\n","        example_text = f\"\"\"User Query: {ex['query']}\\nRetrieved Context:{ex['context']}\\nIdeal Answer: {ex['answer']}---\"\"\"\n","        example_texts.append(example_text)\n","        \n","    prompt_end = f\"\"\"--- Current Task ---\\nUser Query: {user_query}\\nRetrieved Context:{retrieved_context}\\nIdeal Answer:\"\"\"\n","\n","    return prompt_start + \"\\n\".join(example_texts) + prompt_end"]},{"cell_type":"markdown","id":"a92d8f97","metadata":{"papermill":{"duration":0.007496,"end_time":"2025-04-23T22:07:14.550756","exception":false,"start_time":"2025-04-23T22:07:14.54326","status":"completed"},"tags":[]},"source":["# 7. Measuring Success: Evaluating Assistant Quality with AI"]},{"cell_type":"markdown","id":"3a3b9c46","metadata":{"papermill":{"duration":0.006406,"end_time":"2025-04-23T22:07:14.563926","exception":false,"start_time":"2025-04-23T22:07:14.55752","status":"completed"},"tags":[]},"source":["**The Challenge:** We've built an assistant using several techniques, but how do we know if it's actually *good*? Is it accurate? Is it relevant? Is it clear? Manually reviewing every possible answer is impractical.\n","\n","**The Solution: GenAI Evaluation (LLM-as-Judge):** We can leverage another powerful LLM (potentially a more capable one like Gemini Pro) to act as an impartial judge, evaluating the quality of our assistant's responses.\n","\n","**Our Approach:**\n","1.  **Define Criteria:** We establish clear, measurable criteria for what constitutes a \"good\" answer:\n","    * `Faithfulness`: Does the answer accurately reflect *only* the provided rule context? (Crucial for grounding).\n","    * `Relevance`: Does the answer directly address the user's specific question?\n","    * `Clarity`: Is the answer easy to understand?\n","2.  **Create Evaluation Function (`evaluate_ludo_response`):** This function:\n","    * Takes the original query, retrieved context, and the generated answer as input.\n","    * Constructs a *new prompt* specifically instructing the *evaluation model* to assess the generated answer against our criteria.\n","    * Requests the evaluation output in a structured format (JSON) for easy analysis, including scores and justifications.\n","\n","**Why it's Important:**\n","* **Objective Assessment:** Provides a consistent way to measure quality.\n","* **Scalability:** Automates the evaluation process.\n","* **Iterative Improvement:** Evaluation results highlight areas where the assistant (or the underlying retrieval, prompting, etc.) needs refinement.\n","\n","This step adds a crucial quality control layer to our development process."]},{"cell_type":"code","execution_count":11,"id":"19ed749e","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.578035Z","iopub.status.busy":"2025-04-23T22:07:14.577772Z","iopub.status.idle":"2025-04-23T22:07:14.582097Z","shell.execute_reply":"2025-04-23T22:07:14.581175Z"},"papermill":{"duration":0.013302,"end_time":"2025-04-23T22:07:14.5837","exception":false,"start_time":"2025-04-23T22:07:14.570398","status":"completed"},"tags":[]},"outputs":[],"source":["# --- Criteria used to evaluate answer ---\n","EVALUATION_CRITERIA = \"\"\"\n","1.  **Faithfulness (Score 1-5):** Does the answer accurately reflect ONLY the information in the 'Retrieved Context'? (1=Contradicts/Adds Info, 5=Perfectly Faithful).\n","2.  **Relevance (Score 1-5):** Is the answer directly relevant to the 'User Query'? (1=Irrelevant, 5=Highly Relevant).\n","3.  **Clarity (Score 1-5):** Is the answer clear, concise, and easy to understand? (1=Confusing, 5=Very Clear).\n","\"\"\""]},{"cell_type":"code","execution_count":12,"id":"dc503a00","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.598078Z","iopub.status.busy":"2025-04-23T22:07:14.597785Z","iopub.status.idle":"2025-04-23T22:07:14.603564Z","shell.execute_reply":"2025-04-23T22:07:14.602649Z"},"papermill":{"duration":0.014719,"end_time":"2025-04-23T22:07:14.605108","exception":false,"start_time":"2025-04-23T22:07:14.590389","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Gemini 1.5-Pro initialized successfully!\n"]}],"source":["# --- Evaluation Function (using Gemini as Judge) ---\n","try:\n","    # Using Gemini Pro for potentially better evaluation\n","    evaluation_model = genai.GenerativeModel('gemini-1.5-pro') \n","    print(\"Gemini 1.5-Pro initialized successfully!\")\n","except Exception as e:\n","    print(f\"Could not initialize evaluation model: {e}. Using default model.\")\n","    \n","    # Fallback to the main model if Pro isn't available or configured\n","    try:\n","        evaluation_model = genai.GenerativeModel('gemini-1.5-flash')\n","        print(\"Gemini 1.5-Flash initialized successfully!\")\n","    except Exception as e_main:\n","        print(f\"Could not initialize default model either: {e_main}\")\n","        evaluation_model = None"]},{"cell_type":"code","execution_count":13,"id":"9459a4cc","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.620288Z","iopub.status.busy":"2025-04-23T22:07:14.619929Z","iopub.status.idle":"2025-04-23T22:07:14.628305Z","shell.execute_reply":"2025-04-23T22:07:14.627396Z"},"papermill":{"duration":0.017686,"end_time":"2025-04-23T22:07:14.629794","exception":false,"start_time":"2025-04-23T22:07:14.612108","status":"completed"},"tags":[]},"outputs":[],"source":["def evaluate_ludo_response(query, context, generated_answer):\n","    \"\"\"\n","    Uses an LLM to evaluate the quality of the generated Ludo rule explanation.\n","    Returns a dictionary with scores and justifications, or an error dict.\n","    \"\"\"\n","    if not evaluation_model:\n","        print(\"Evaluation model not available.\")\n","        return {\"error\": \"Evaluation model not initialized\"}\n","\n","    print(f\"\\n--- Evaluating Response ---\")\n","    print(f\"Query: {query}\")\n","    print(f\"Context:\\n{context}\")\n","    print(f\"Generated Answer:\\n{generated_answer}\")\n","\n","    eval_prompt = f\"\"\"You are an impartial evaluator assessing the quality of an AI-generated answer about Ludo rules.\n","Evaluate the 'Generated Answer' based on the 'User Query' and the 'Retrieved Context' it was supposed to be based on.\n","\n","Use the following criteria:\n","{EVALUATION_CRITERIA}\n","\n","--- Input Data ---\n","User Query: {query}\n","\n","Retrieved Context:\n","{context}\n","\n","Generated Answer:\n","{generated_answer}\n","\n","--- Evaluation Output ---\n","Provide a score (1-5) for each criterion (Faithfulness, Relevance, Clarity) and a brief justification for each score. Format the output as a JSON object. Example:\n","```json\n","{{\n","  \"faithfulness_score\": 5,\n","  \"faithfulness_justification\": \"Accurately reflects context.\",\n","  \"relevance_score\": 5,\n","  \"relevance_justification\": \"Directly answers query.\",\n","  \"clarity_score\": 4,\n","  \"clarity_justification\": \"Clear, slightly wordy.\"\n","}}\n","\n","Evaluation Output (JSON):\n","\"\"\"\n","\n","    evaluation_response = None\n","    \n","    try:\n","        evaluation_response = evaluation_model.generate_content(eval_prompt, generation_config=genai.types.GenerationConfig(\n","            # Ensure it outputs JSON reliably\n","            response_mime_type=\"application/json\",\n","            temperature=0.1 # Low temperature for consistent evaluation\n","            ) \n","        )\n","        # Accessing JSON directly if response_mime_type=\"application/json\" is used and successful\n","        eval_result_json = json.loads(evaluation_response.text)\n","        print(f\"--- Evaluation Result (JSON) ---\")\n","        print(json.dumps(eval_result_json, indent=2))\n","        return eval_result_json\n","        \n","    # Handle cases where the response wasn't valid JSON\n","    except ValueError as json_err:\n","        print(f\"Evaluation JSON Parsing Error: {json_err}\")\n","        if evaluation_response: print(f\"Raw evaluation response text: {evaluation_response.text}\")\n","        return {\"error\": \"Failed to parse evaluation JSON\", \"raw_text\": evaluation_response.text if evaluation_response else \"N/A\"}\n","    except Exception as e:\n","        print(f\"Evaluation Error: {e}\")\n","        # Log feedback if available (e.g., blocked content)\n","        try:\n","            if evaluation_response: print(f\"Evaluation Prompt Feedback: {evaluation_response.prompt_feedback}\")\n","        except: \n","            pass # Ignore if feedback isn't available\n","        return {\"error\": f\"Failed to get or parse evaluation: {e}\"}"]},{"cell_type":"markdown","id":"80a962f8","metadata":{"papermill":{"duration":0.006451,"end_time":"2025-04-23T22:07:14.643126","exception":false,"start_time":"2025-04-23T22:07:14.636675","status":"completed"},"tags":[]},"source":["# 8. Orchestrating the Process: The Main Workflow Function"]},{"cell_type":"markdown","id":"5730fdbf","metadata":{"papermill":{"duration":0.006233,"end_time":"2025-04-23T22:07:14.655885","exception":false,"start_time":"2025-04-23T22:07:14.649652","status":"completed"},"tags":[]},"source":["**The Goal:** Now we bring all the previously defined components together into a single, executable workflow that handles an incoming user query from start to finish.\n","\n","**The Solution: `handle_ludo_query` Function:** This function acts as the central \"engine\" for our Ludo assistant. It orchestrates the sequence of operations:\n","1.  **Input:** Takes the user's query (`user_query`).\n","2.  **Retrieve (RAG):** Calls `retrieve_relevant_rules` to get factual context from the KB. Handles cases where no relevant context is found.\n","3.  **Format Prompt:** Calls `format_prompt_with_all_features` to assemble the comprehensive prompt, including instructions, grounding directives, few-shot examples, the query, and the retrieved context.\n","4.  **Generate & Handle Functions:** Calls the main Gemini model (`chat_model`) with the formatted prompt and enabled tools (for function calling). It checks if Gemini requests a function call (like `clarify_ludo_term`).\n","    * *If Function Call Requested:* Executes the appropriate Python function, sends the result back to Gemini, and gets the final text answer incorporating that result.\n","    * *If No Function Call:* Processes the direct text response from Gemini.\n","5.  **Extract Answer:** Obtains the final generated text response. Includes error handling for API issues or empty responses.\n","6.  **Evaluate (Optional):** If requested (`evaluate=True`), it calls the `evaluate_ludo_response` function to get an AI-based quality assessment of the generated answer.\n","7.  **Output:** Returns the final answer string.\n","\n","This function demonstrates the practical integration and execution of all the GenAI techniques discussed earlier to solve the user's Ludo query."]},{"cell_type":"code","execution_count":14,"id":"d97fa06a","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.670816Z","iopub.status.busy":"2025-04-23T22:07:14.670499Z","iopub.status.idle":"2025-04-23T22:07:14.683828Z","shell.execute_reply":"2025-04-23T22:07:14.682895Z"},"papermill":{"duration":0.02256,"end_time":"2025-04-23T22:07:14.685173","exception":false,"start_time":"2025-04-23T22:07:14.662613","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Main chat model (gemini-1.5-flash) initialized with tools.\n"]}],"source":["#--- Initialize the main Gemini Model ---\n","# Usinging flash here for potentially faster responses in the main task\n","main_model_name = 'gemini-1.5-flash'\n","chat_model = None # Initialized as None\n","if GOOGLE_API_KEY:\n","    try:\n","        # Model used for the main generation task, WITH tools enabled\n","        chat_model = genai.GenerativeModel(main_model_name, tools=[ludo_tool])\n","        print(f\"Main chat model ({main_model_name}) initialized with tools.\")\n","    except Exception as e:\n","        print(f\"FATAL: Could not initialize main chat model {main_model_name}: {e}\")\n","        # chat_model remains None\n","else:\n","    print(\"Skipping main model initialization - API Key not found.\")\n","\n","def handle_ludo_query(user_query, evaluate=False):\n","    \"\"\"\n","    Processes a user query about Ludo rules using all integrated techniques.\n","    Args:\n","    user_query (str): The question asked by the user.\n","    evaluate (bool): Whether to run the LLM-as-Judge evaluation.\n","    Returns:\n","    str: The final generated answer.\n","    (Or dict containing answer and eval results if evaluate=True - adjust as needed)\n","    \"\"\"\n","    if not chat_model:\n","        return \"Error: Main chat model is not available. Please configure API Key.\"\n","\n","    print(f\"\\n=== Handling Query: '{user_query}' ===\")\n","\n","    # 1. RAG: Retrieve relevant context\n","    retrieved_context = retrieve_relevant_rules(user_query, LUDO_RULES_KB)\n","    if not retrieved_context or retrieved_context == \"No specific rules found matching the query keywords.\":\n","       print(\"WARNING: Could not find relevant rules in KB for grounding.\")\n","       # Provide minimal context indicating lack of specific info\n","       retrieved_context = \"No specific rules were found in the knowledge base matching the query keywords.\"\n","    \n","    # 2. Format Prompt (Few-Shot, Grounding)\n","    prompt = format_prompt_with_all_features(user_query, retrieved_context, few_shot_examples)\n","    # print(f\"\\n--- Generated Prompt Snippet ---\\n{prompt[:500]}...\\n--- End Snippet ---\") # Debug\n","    \n","    # 3. Generation Attempt 1 (Checking for Function Calls)\n","    final_answer = \"Sorry, I encountered an issue generating the response.\" # Default error msg\n","    response = None # Define response variable for broader scope error logging\n","    try:\n","        # Start a chat session for potential multi-turn function calling\n","        # Note: For simple single-turn function calls, chat might be overkill, but it's robust for handling the back-and-forth.\n","        chat = chat_model.start_chat(enable_automatic_function_calling=False) # Manual control\n","        \n","        response = chat.send_message(prompt)\n","        \n","        # Check if the model decided to call a function\n","        # Gemini API structure check: response.candidates[0].content.parts[0].function_call\n","        candidate = response.candidates[0]\n","        if candidate.content.parts and candidate.content.parts[0].function_call:\n","            fc = candidate.content.parts[0].function_call\n","            print(f\"DEBUG: Gemini wants to call function: {fc.name} with args: {dict(fc.args)}\")\n","            \n","            # Find the Python function object based on the name Gemini provided\n","            function_to_call = globals().get(fc.name) \n","            \n","            if function_to_call:\n","                 # 4. Execute the Function\n","                function_args = dict(fc.args)\n","                try:\n","                    function_result = function_to_call(**function_args) # Call the actual Python func\n","                    print(f\"DEBUG: Function {fc.name} executed. Result: {function_result}\")\n","    \n","                    # 5. Send Function Result Back to Gemini\n","                    # Use the chat history to send the result back correctly\n","                    response = chat.send_message(\n","                        Part.from_function_response(\n","                            name=fc.name,\n","                            response=function_result # Pass the direct return value (dict)\n","                        )\n","                    )\n","                    # The model generates the final text response after getting the function result\n","                    if response.parts:\n","                        final_answer = response.text\n","                    else:\n","                         final_answer = \"I received the function result but couldn't generate a final text answer.\"\n","                         print(f\"Error: No text part after function call. Feedback: {response.prompt_feedback}\")\n","    \n","                except Exception as func_exec_err:\n","                    print(f\"ERROR executing function {fc.name}: {func_exec_err}\")\n","                    final_answer = f\"I tried to use my tool '{fc.name}' but encountered an execution error.\"\n","            else:\n","                print(f\"ERROR: Declared function '{fc.name}' not found in Python code.\")\n","                final_answer = f\"I tried to use a tool '{fc.name}' but couldn't find it.\"\n","        \n","        # If no function call was requested in the first place\n","        elif response.parts:\n","            final_answer = response.text\n","        else:\n","            # Handle cases where the initial response might be blocked or empty\n","            print(f\"Error: No function call and no text part in initial response. Feedback: {response.prompt_feedback}\")\n","            final_answer = \"Sorry, I couldn't generate a response based on the provided information.\"\n","    \n","    except Exception as e:\n","        print(f\"An error occurred during generation or function handling: {e}\")\n","        # Log feedback if available\n","        try:\n","            if response: print(f\"Prompt Feedback: {response.prompt_feedback}\")\n","        except: pass\n","        final_answer = f\"Sorry, a technical error occurred: {e}\"\n","    \n","    # --- Output Final Answer ---\n","    print(f\"\\n--- Final Generated Answer ---\")\n","    print(final_answer)\n","    \n","    # 6. Evaluation (Optional)\n","    evaluation_result = None\n","    if evaluate:\n","        # Check if the evaluation model is ready before calling\n","        if evaluation_model:\n","            evaluation_result = evaluate_ludo_response(user_query, retrieved_context, final_answer)\n","        else:\n","            print(\"Skipping evaluation because the evaluation model is not available.\")\n","            evaluation_result = {\"error\": \"Evaluation model not available\"}\n","        # You might return both answer and evaluation, or just log eval results\n","        \n","    # For simplicity, just return the answer string here\n","    return final_answer # Or potentially return a dict: {'answer': final_answer, 'evaluation': evaluation_result}"]},{"cell_type":"markdown","id":"dd239f9a","metadata":{"papermill":{"duration":0.006531,"end_time":"2025-04-23T22:07:14.698404","exception":false,"start_time":"2025-04-23T22:07:14.691873","status":"completed"},"tags":[]},"source":["# 9. Putting it to the Test: Running Example Queries"]},{"cell_type":"markdown","id":"dc5cf63b","metadata":{"papermill":{"duration":0.006468,"end_time":"2025-04-23T22:07:14.711463","exception":false,"start_time":"2025-04-23T22:07:14.704995","status":"completed"},"tags":[]},"source":["**The Purpose:** Having built the workflow, let's see how our Ludo assistant performs in practice. We'll run several example queries designed to test different aspects of its functionality.\n","\n","**Test Cases:**\n","* **Basic Rule Recall:** Simple questions to see if it can retrieve and explain core rules (e.g., \"How do I win?\").\n","* **Function Call Trigger:** Queries that might involve ambiguous terms, potentially prompting the AI to use the `clarify_ludo_term` function (e.g., \"What does 'capture' mean?\").\n","* **Grounding Limits:** Questions asking for information likely *not* in our knowledge base (e.g., strategic advice) to see if the assistant correctly states it cannot answer from the provided context.\n","* **Specific Scenarios:** Questions about interactions between rules (e.g., \"Can I be captured on a star square?\").\n","* **Complex Interactions:** Queries requiring understanding of multiple rules simultaneously (e.g., blocks affecting safe squares).\n","\n","**Observation:** By running these diverse queries and examining the generated answers (and optional evaluations), we can gain insights into the assistant's strengths and weaknesses, identifying areas for further refinement of the knowledge base, retrieval, prompting, or function logic."]},{"cell_type":"code","execution_count":15,"id":"bf7dbc1d","metadata":{"execution":{"iopub.execute_input":"2025-04-23T22:07:14.726344Z","iopub.status.busy":"2025-04-23T22:07:14.725524Z","iopub.status.idle":"2025-04-23T22:07:23.596316Z","shell.execute_reply":"2025-04-23T22:07:23.595124Z"},"papermill":{"duration":8.880165,"end_time":"2025-04-23T22:07:23.59807","exception":false,"start_time":"2025-04-23T22:07:14.717905","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","=== Handling Query: 'How do I win Ludo?' ===\n","DEBUG: Retrieved 1 snippets for query 'How do I win Ludo?'\n","\n","--- Final Generated Answer ---\n","The provided text does not explain how to win Ludo.\n","\n","\n","--- Evaluating Response ---\n","Query: How do I win Ludo?\n","Context:\n","- A block can be moved forward together if the player rolls an even number. The die roll is divided by two, and the block moves that many spaces. For example, rolling a 4 allows the block to move 2 spaces. Some variations do not allow moving blocks.\n","Generated Answer:\n","The provided text does not explain how to win Ludo.\n","\n","--- Evaluation Result (JSON) ---\n","{\n","  \"faithfulness_score\": 5,\n","  \"faithfulness_justification\": \"The answer accurately reflects the provided context, which does not describe how to win at Ludo.\",\n","  \"relevance_score\": 5,\n","  \"relevance_justification\": \"The answer is perfectly relevant to the query as it states that the provided text does not contain the answer.\",\n","  \"clarity_score\": 5,\n","  \"clarity_justification\": \"The answer is clear, concise, and easy to understand.\"\n","}\n","\n","==================================================\n","\n","\n","=== Handling Query: 'What does 'capture' mean exactly in the rules?' ===\n","DEBUG: Retrieved 3 snippets for query 'What does 'capture' mean exactly in the rules?'\n","\n","--- Final Generated Answer ---\n","The provided text does not contain a definition for the term 'capture' in Ludo.\n","\n","\n","==================================================\n","\n","\n","=== Handling Query: 'What's the best opening strategy?' ===\n","DEBUG: Retrieved 3 snippets for query 'What's the best opening strategy?'\n","\n","--- Final Generated Answer ---\n","The provided text describes the goal of the game and the initial setup, but it does not offer any strategies for the opening moves.  Therefore, I cannot answer your question.\n","\n","\n","--- Evaluating Response ---\n","Query: What's the best opening strategy?\n","Context:\n","- The main goal of Ludo is to be the first player to move all four of their tokens (pieces) from their starting area, around the entire board track, and into the central home triangle.\n","- Each player chooses a color and places their four tokens in the corresponding starting circle (yard). The game board is set up with the track, starting squares, safe squares, and home columns.\n","- A player rolls the die once per turn, unless they roll a 6, which grants an additional roll (bonus roll).\n","Generated Answer:\n","The provided text describes the goal of the game and the initial setup, but it does not offer any strategies for the opening moves.  Therefore, I cannot answer your question.\n","\n","--- Evaluation Result (JSON) ---\n","{\n","  \"faithfulness_score\": 5,\n","  \"faithfulness_justification\": \"The answer correctly states that the provided text does not contain information about opening strategies.\",\n","  \"relevance_score\": 4,\n","  \"relevance_justification\": \"While the answer acknowledges the inability to answer the query due to insufficient context, it remains relevant to the user's original question about opening strategies.\",\n","  \"clarity_score\": 5,\n","  \"clarity_justification\": \"The response is clear, concise, and easy to understand.\"\n","}\n","\n","==================================================\n","\n","\n","=== Handling Query: 'Can I be captured on a star square?' ===\n","DEBUG: Retrieved 3 snippets for query 'Can I be captured on a star square?'\n","\n","--- Final Generated Answer ---\n","No, a token cannot be captured if it is on a safe square, which are often marked with a star or globe.\n","\n","\n","==================================================\n","\n","\n","=== Handling Query: 'How do blocks affect capturing on safe squares?' ===\n","DEBUG: Retrieved 3 snippets for query 'How do blocks affect capturing on safe squares?'\n","\n","--- Final Generated Answer ---\n","Blocks cannot be formed on safe squares.  A piece on a safe square cannot be captured.\n","\n","\n","--- Evaluating Response ---\n","Query: How do blocks affect capturing on safe squares?\n","Context:\n","- When two or more tokens of the same color occupy the same non-safe square, they form a 'block'. An opponent's token cannot pass or land on a square occupied by a block. Blocks cannot be formed on safe squares.\n","- If a player's token lands on a square occupied by a single opponent's token (not on a safe square), the opponent's token is captured. Captured tokens are returned to their owner's starting circle (yard) and must be brought back into play by rolling a 6.\n","- Certain squares on the board are designated as 'safe squares'. These typically include the starting square for each color and squares marked with a star or globe. An opponent cannot capture a token resting on a safe square. Multiple tokens (even from different players) can occupy a safe square.\n","Generated Answer:\n","Blocks cannot be formed on safe squares.  A piece on a safe square cannot be captured.\n","\n","--- Evaluation Result (JSON) ---\n","{\n","  \"faithfulness_score\": 5,\n","  \"faithfulness_justification\": \"The generated answer accurately reflects the information provided in the context. It correctly states that blocks cannot be formed on safe squares and that pieces on safe squares cannot be captured.\",\n","  \"relevance_score\": 5,\n","  \"relevance_justification\": \"The generated answer directly addresses the user's query about how blocks affect capturing on safe squares by explaining that blocks cannot be formed on safe squares and pieces on safe squares are immune to capture.\",\n","  \"clarity_score\": 5,\n","  \"clarity_justification\": \"The answer is clear, concise, and easy to understand. It uses simple language and avoids unnecessary jargon.\"\n","}\n","\n","==================================================\n","\n"]}],"source":["if chat_model:\n","    # --- Example 1: Basic Rule Question ---\n","    query1 = \"How do I win Ludo?\"\n","    answer1 = handle_ludo_query(query1, evaluate=True)\n","    # Enable evaluation for this one\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","\n","    # --- Example 2: Question likely triggering Function Call ---\n","    query2 = \"What does 'capture' mean exactly in the rules?\" \n","    # This might trigger clarify_ludo_term if Gemini thinks the context needs supplement\n","    answer2 = handle_ludo_query(query2, evaluate=False) # Evaluation disabled\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","    \n","    # --- Example 3: Question testing Grounding (Answer likely not in KB) ---\n","    query3 = \"What's the best opening strategy?\" \n","    # KB doesn't contain strategy advice, should state that\n","    answer3 = handle_ludo_query(query3, evaluate=True)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","    \n","    # --- Example 4: Question about safe squares ---\n","    query4 = \"Can I be captured on a star square?\"\n","    answer4 = handle_ludo_query(query4, evaluate=False)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","    \n","    # --- Example 5: More complex question potentially needing clarification ---\n","    query5 = \"How do blocks affect capturing on safe squares?\" \n","    # Might retrieve multiple rules, clarity is key\n","    answer5 = handle_ludo_query(query5, evaluate=True)\n","    print(\"\\n\" + \"=\"*50 + \"\\n\")\n","else:\n","    print(\"Cannot run examples because the main chat model failed to initialize.\")"]},{"cell_type":"markdown","id":"56bf82cc","metadata":{"papermill":{"duration":0.007077,"end_time":"2025-04-23T22:07:23.612841","exception":false,"start_time":"2025-04-23T22:07:23.605764","status":"completed"},"tags":[]},"source":["# 10. Conclusion and Key Learnings"]},{"cell_type":"markdown","id":"b4a9e3b8","metadata":{"papermill":{"duration":0.007744,"end_time":"2025-04-23T22:07:23.627693","exception":false,"start_time":"2025-04-23T22:07:23.619949","status":"completed"},"tags":[]},"source":["**Summary:** This notebook successfully demonstrated the creation of a Ludo Rules Assistant by leveraging Google's Gemini model combined with several powerful techniques. We addressed the core problem – the need for clear, accurate Ludo rule explanations – by implementing:\n","* **RAG:** To ground answers in a factual knowledge base.\n","* **Few-Shot Prompting:** To ensure responses are consistently clear and well-formatted.\n","* **Function Calling:** To handle specific tasks like term clarification, making the assistant more robust.\n","* **Prompt-Based Grounding:** To minimize hallucinations and ensure reliability.\n","* **GenAI Evaluation:** To systematically assess the quality of the generated answers.\n","\n","**Key Learnings:** Building a high-quality, domain-specific AI assistant often requires more than just basic prompting. Integrating techniques like RAG, function calling, and careful prompt engineering (including few-shot examples and grounding) significantly enhances accuracy, reliability, and helpfulness. Evaluation is critical for understanding performance and guiding improvements."]},{"cell_type":"markdown","id":"2ee84baa","metadata":{"papermill":{"duration":0.008351,"end_time":"2025-04-23T22:07:23.643686","exception":false,"start_time":"2025-04-23T22:07:23.635335","status":"completed"},"tags":[]},"source":["---\n","This notebook was developed by **Asmit Patidar**."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":26.144862,"end_time":"2025-04-23T22:07:26.281073","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-23T22:07:00.136211","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}
